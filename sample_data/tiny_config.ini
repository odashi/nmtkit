[Global]
forward_memory_mb=256
backward_memory_mb=256
parameter_memory_mb=256
random_seed=12345
backend_random_seed=12345
archive_format=binary

[Corpus]
train_source=sample_data/tiny.in
train_target=sample_data/tiny.out
dev_source=sample_data/tiny.in
dev_target=sample_data/tiny.out
test_source=sample_data/tiny.in
test_target=sample_data/tiny.out

[Model]
source_vocabulary_type=word
target_vocabulary_type=word
unk_frequency=0
source_vocabulary_size=33
target_vocabulary_size=33
encoder_type=bidirectional
decoder_type=default
num_layers=1
source_embedding_size=67
target_embedding_size=68
output_embedding_size=69
encoder_hidden_size=63
decoder_hidden_size=64
attention_type=mlp
attention_hidden_size=62
predictor_type=softmax

[Batch]
batch_method=target_word
sort_method=target_source
batch_size=32
max_length=16
max_length_ratio=1.5

[Train]
optimizer_type=sgd
sgd_eta=0.1
loss_integration_type=sum
lr_decay_type=none
lr_decay_ratio=1.0
dropout_ratio=0.0
max_iteration=1000
evaluation_type=step
evaluation_interval=100
